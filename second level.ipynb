{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, model_selection\n",
    "import string\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from scipy.stats import boxcox\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier , ExtraTreesClassifier , AdaBoostClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = 'train.json'\n",
    "test_file = 'test.json'\n",
    "train = pd.read_json(train_file)\n",
    "test = pd.read_json(test_file)\n",
    "listing_id = test.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "train['interest_level'] = train['interest_level'].apply(lambda x: y_map[x])\n",
    "y_train = train.interest_level.values\n",
    "\n",
    "train = train.drop(['listing_id', 'interest_level'], axis=1)\n",
    "test = test.drop('listing_id', axis=1)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "\n",
    "train_test = pd.concat((train, test), axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['Date'] = pd.to_datetime(train_test['created'])\n",
    "train_test['Year'] = train_test['Date'].dt.year\n",
    "train_test['Month'] = train_test['Date'].dt.month\n",
    "train_test['Day'] = train_test['Date'].dt.day\n",
    "train_test['Wday'] = train_test['Date'].dt.dayofweek\n",
    "train_test['Yday'] = train_test['Date'].dt.dayofyear\n",
    "train_test['hour'] = train_test['Date'].dt.hour\n",
    "\n",
    "train_test = train_test.drop(['Date', 'created'], axis=1)\n",
    "\n",
    "train_test['Zero_building_id'] = train_test['building_id'].apply(lambda x: 1 if x == '0' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['desc'] = train_test['description']\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "\n",
    "string.punctuation.__add__('!!')\n",
    "string.punctuation.__add__('(')\n",
    "string.punctuation.__add__(')')\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.translate(remove_punct_map))\n",
    "train_test['desc_letters_count'] = train_test['description'].apply(lambda x: len(x.strip()))\n",
    "train_test['desc_words_count'] = train_test['desc'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "\n",
    "train_test.drop(['description', 'desc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['address1'] = train_test['display_address']\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "address_map = {\n",
    "    'w': 'west',\n",
    "    'st.': 'street',\n",
    "    'ave': 'avenue',\n",
    "    'st': 'street',\n",
    "    'e': 'east',\n",
    "    'n': 'north',\n",
    "    's': 'south'\n",
    "}\n",
    "\n",
    "\n",
    "def address_map_func(s):\n",
    "    s = s.split(' ')\n",
    "    out = []\n",
    "    for x in s:\n",
    "        if x in address_map:\n",
    "            out.append(address_map[x])\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return ' '.join(out)\n",
    "\n",
    "\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: x.translate(remove_punct_map))\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: address_map_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "\n",
    "for col in new_cols:\n",
    "    train_test[col] = train_test['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "train_test['other_address'] = train_test[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "\n",
    "train_test['features_count'] = train_test['features'].apply(lambda x: len(x))\n",
    "train_test['features2'] = train_test['features']\n",
    "train_test['features2'] = train_test['features2'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "c_vect.fit(train_test['features2'])\n",
    "\n",
    "c_vect_sparse_1 = c_vect.transform(train_test['features2'])\n",
    "c_vect_sparse1_cols = c_vect.get_feature_names()\n",
    "\n",
    "train_test.drop(['features', 'features2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "managers_count = train_test['manager_id'].value_counts()\n",
    "\n",
    "train_test['top_10_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 90)] else 0)\n",
    "train_test['top_25_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 75)] else 0)\n",
    "train_test['top_5_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 95)] else 0)\n",
    "train_test['top_50_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 50)] else 0)\n",
    "train_test['top_1_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 99)] else 0)\n",
    "train_test['top_2_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 98)] else 0)\n",
    "train_test['top_15_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 85)] else 0)\n",
    "train_test['top_20_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 80)] else 0)\n",
    "train_test['top_30_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 70)] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings_count = train_test['building_id'].value_counts()\n",
    "\n",
    "train_test['top_10_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "train_test['top_25_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "train_test['top_5_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "train_test['top_50_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "train_test['top_1_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "train_test['top_2_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "train_test['top_15_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "train_test['top_20_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "train_test['top_30_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['photos_count'] = train_test['photos'].apply(lambda x: len(x))\n",
    "train_test.drop(['photos', 'display_address', 'street_address'], axis=1, inplace=True)\n",
    "\n",
    "categoricals = [x for x in train_test.columns if train_test[x].dtype == 'object']\n",
    "\n",
    "for feat in categoricals:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_test[feat].values))\n",
    "    train_test[feat] = lbl.transform(list(train_test[feat].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc_price, tmp = boxcox(train_test.price)\n",
    "train_test['bc_price'] = bc_price\n",
    "\n",
    "train_test.drop('price', axis=1, inplace=True)\n",
    "train_test['bathrooms_cat'] = train_test['bathrooms'].apply(lambda x: str(x))\n",
    "\n",
    "train_test['bathrooms_cat'], labels = pd.factorize(train_test['bathrooms_cat'].values, sort=True)\n",
    "train_test.drop('bathrooms', axis=1, inplace=True)\n",
    "\n",
    "train_test['bedroom_cat'], labels = pd.factorize(train_test['bedrooms'].values, sort=True)\n",
    "train_test.drop('bedrooms', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(train_test.columns)\n",
    "train_test_cv1_sparse = sparse.hstack((train_test, c_vect_sparse_1)).tocsr()\n",
    "x_train = train_test_cv1_sparse[:ntrain, :]\n",
    "x_test = train_test_cv1_sparse[ntrain:, :]\n",
    "features += c_vect_sparse1_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 777\n",
    "NFOLDS = 5\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=25)\n",
    "# best_rounds = np.argmin(bst['test-mlogloss-mean']\n",
    "# bst = xgb.train(params, dtrain, best_rounds)\n",
    "# preds = bst.predict(dtest)\n",
    "# preds = pd.DataFrame(preds)\n",
    "# cols = ['high', 'medium', 'low']\n",
    "# preds.columns = cols\n",
    "# preds['listing_id'] = listing_id\n",
    "# preds.to_csv('my_preds.csv', index=None)\n",
    "\n",
    "#instead load the pickle model\n",
    "with open('xgboost.pkl', 'rb') as xgboost:\n",
    "    bst = pickle.load(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47050505216533894"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred1= bst.predict(dtrain)\n",
    "train_pred1 = pd.DataFrame(train_pred1)\n",
    "cols = ['high', 'medium', 'low']\n",
    "train_pred1.columns = cols\n",
    "train_loss = log_loss(y_train,train_pred1)\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37498744907016018"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf1 = RandomForestClassifier(n_estimators=1000, criterion='entropy',  n_jobs = -1)\n",
    "# rf1.fit(x_train, y_train)\n",
    "# y_train_pred2 = rf1.predict_proba(x_train)\n",
    "# log_loss(y_train, y_train_pred2)\n",
    "\n",
    "#load the second model rf with entropy\n",
    "with open('rf1.pkl', 'rb') as rf1:\n",
    "    rf1 = pickle.load(rf1)\n",
    "y_train_pred2 = rf1.predict_proba(x_train)\n",
    "log_loss(y_train, y_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37498744907016018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf2 = RandomForestClassifier(n_estimators=1000, criterion='gini',  n_jobs = -1)\n",
    "# rf2.fit(x_train, y_train)\n",
    "# y_train_pred2 = rf2.predict_proba(x_train)\n",
    "# log_loss(y_train, y_train_pred2)\n",
    "\n",
    "\n",
    "#load the third model rf with gini\n",
    "with open('rf2.pkl', 'rb') as rf2:\n",
    "    rf2 = pickle.load(rf2)\n",
    "y_train_pred3 = rf2.predict_proba(x_train)\n",
    "log_loss(y_train, y_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52720866312146875"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gbc = GradientBoostingClassifier(n_estimators=1000)\n",
    "# gbc.fit(x_train, y_train)\n",
    "\n",
    "#load the fourth model gbc\n",
    "with open('gbc.pkl', 'rb') as gbc:\n",
    "    gbc = pickle.load(gbc)\n",
    "\n",
    "\n",
    "x_train1 = x_train.toarray()\n",
    "y_train_pred4 = gbc.predict_proba(x_train1)\n",
    "log_loss(y_train, y_train_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74201032684774915"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# etc = ExtraTreesClassifier(n_estimators=1100, min_samples_leaf=100,max_depth=4, n_jobs=-1, random_state=104743, verbose=0)\n",
    "# etc.fit(x_train, y_train)\n",
    "\n",
    "# Model 5 extra trees classifier\n",
    "with open('etc.pkl', 'rb') as etc:\n",
    "    etc = pickle.load(etc)\n",
    "\n",
    "y_train_pred5 = etc.predict_proba(x_train)\n",
    "log_loss(y_train, y_train_pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.096411162128982"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ada = AdaBoostClassifier(n_estimators=1100)\n",
    "# ada.fit(x_train,y_train)\n",
    "\n",
    "#model 6 ada boost\n",
    "\n",
    "with open('ada.pkl', 'rb') as ada:\n",
    "    ada = pickle.load(ada)\n",
    "\n",
    "\n",
    "y_train_pred6 = ada.predict_proba(x_train)\n",
    "log_loss(y_train, y_train_pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = np.concatenate((train_pred1,y_train_pred2,y_train_pred3,y_train_pred4,y_train_pred5,y_train_pred6),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "dtrain_res = xgb.DMatrix(data=results, label=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08474+6.21611e-06\ttest-mlogloss:1.08483+5.6356e-06\n",
      "[25]\ttrain-mlogloss:0.805688+0.000286049\ttest-mlogloss:0.807839+0.000329037\n",
      "[50]\ttrain-mlogloss:0.615109+0.000224199\ttest-mlogloss:0.619027+0.000322942\n",
      "[75]\ttrain-mlogloss:0.47912+0.000261068\ttest-mlogloss:0.484572+0.000393588\n",
      "[100]\ttrain-mlogloss:0.379352+0.000274115\ttest-mlogloss:0.386227+0.000485102\n",
      "[125]\ttrain-mlogloss:0.304762+0.000279085\ttest-mlogloss:0.312932+0.000609099\n",
      "[150]\ttrain-mlogloss:0.248219+0.000282713\ttest-mlogloss:0.257628+0.000745643\n",
      "[175]\ttrain-mlogloss:0.204942+0.000288424\ttest-mlogloss:0.215504+0.000844449\n",
      "[200]\ttrain-mlogloss:0.171446+0.000312214\ttest-mlogloss:0.183132+0.000926455\n",
      "[225]\ttrain-mlogloss:0.145369+0.000358824\ttest-mlogloss:0.158076+0.000983585\n",
      "[250]\ttrain-mlogloss:0.124882+0.000368016\ttest-mlogloss:0.138622+0.00106131\n",
      "[275]\ttrain-mlogloss:0.108648+0.000391561\ttest-mlogloss:0.123385+0.00113041\n",
      "[300]\ttrain-mlogloss:0.0957126+0.00038936\ttest-mlogloss:0.1114+0.00121355\n",
      "[325]\ttrain-mlogloss:0.0853702+0.000401642\ttest-mlogloss:0.101975+0.00123733\n",
      "[350]\ttrain-mlogloss:0.077026+0.000420441\ttest-mlogloss:0.0945156+0.00129436\n",
      "[375]\ttrain-mlogloss:0.0702692+0.000434073\ttest-mlogloss:0.088569+0.00135006\n",
      "[400]\ttrain-mlogloss:0.0647462+0.000446091\ttest-mlogloss:0.0837932+0.00142811\n",
      "[425]\ttrain-mlogloss:0.0601882+0.000445415\ttest-mlogloss:0.0799392+0.001481\n",
      "[450]\ttrain-mlogloss:0.0564284+0.000446323\ttest-mlogloss:0.076806+0.00155848\n",
      "[475]\ttrain-mlogloss:0.0532454+0.000448949\ttest-mlogloss:0.074214+0.00162072\n",
      "[500]\ttrain-mlogloss:0.05055+0.000434333\ttest-mlogloss:0.0720358+0.00167159\n",
      "[525]\ttrain-mlogloss:0.0482556+0.00043182\ttest-mlogloss:0.0702406+0.00172596\n",
      "[550]\ttrain-mlogloss:0.046257+0.000432254\ttest-mlogloss:0.0687238+0.00174822\n",
      "[575]\ttrain-mlogloss:0.0444836+0.000415963\ttest-mlogloss:0.0674196+0.00177703\n",
      "[600]\ttrain-mlogloss:0.042829+0.000410735\ttest-mlogloss:0.0662438+0.00179377\n",
      "[625]\ttrain-mlogloss:0.0413102+0.000437465\ttest-mlogloss:0.0652208+0.00181397\n",
      "[650]\ttrain-mlogloss:0.0399118+0.00043469\ttest-mlogloss:0.0643032+0.00184247\n",
      "[675]\ttrain-mlogloss:0.0385982+0.00043909\ttest-mlogloss:0.063476+0.00186345\n",
      "[700]\ttrain-mlogloss:0.0373946+0.000451376\ttest-mlogloss:0.062766+0.00187023\n",
      "[725]\ttrain-mlogloss:0.0362516+0.000474406\ttest-mlogloss:0.0621284+0.00190063\n",
      "[750]\ttrain-mlogloss:0.0351846+0.000455615\ttest-mlogloss:0.0615406+0.00194886\n",
      "[775]\ttrain-mlogloss:0.034162+0.000470775\ttest-mlogloss:0.0610026+0.00197411\n",
      "[800]\ttrain-mlogloss:0.0331796+0.000464835\ttest-mlogloss:0.0605096+0.00200959\n",
      "[825]\ttrain-mlogloss:0.0322572+0.000475627\ttest-mlogloss:0.060075+0.00204889\n",
      "[850]\ttrain-mlogloss:0.0313898+0.000460801\ttest-mlogloss:0.0596784+0.00207919\n",
      "[875]\ttrain-mlogloss:0.0305794+0.000468102\ttest-mlogloss:0.0593374+0.00210176\n",
      "[900]\ttrain-mlogloss:0.0297958+0.00045301\ttest-mlogloss:0.0590162+0.00213028\n",
      "[925]\ttrain-mlogloss:0.0290538+0.000458756\ttest-mlogloss:0.058725+0.00214881\n",
      "[950]\ttrain-mlogloss:0.0283332+0.000461507\ttest-mlogloss:0.0584634+0.00217817\n",
      "[975]\ttrain-mlogloss:0.0276414+0.000447508\ttest-mlogloss:0.0581976+0.00220423\n",
      "[1000]\ttrain-mlogloss:0.0270068+0.000460903\ttest-mlogloss:0.0579876+0.00222675\n",
      "[1025]\ttrain-mlogloss:0.0263892+0.000465393\ttest-mlogloss:0.0577912+0.00227015\n",
      "[1050]\ttrain-mlogloss:0.0257974+0.000476677\ttest-mlogloss:0.057622+0.00229498\n",
      "[1075]\ttrain-mlogloss:0.0252018+0.000471319\ttest-mlogloss:0.0574618+0.0023236\n",
      "[1100]\ttrain-mlogloss:0.0246264+0.000483353\ttest-mlogloss:0.057311+0.00234259\n",
      "[1125]\ttrain-mlogloss:0.0240904+0.00047588\ttest-mlogloss:0.0571866+0.00235646\n",
      "[1150]\ttrain-mlogloss:0.0235776+0.000467378\ttest-mlogloss:0.0570576+0.00237874\n",
      "[1175]\ttrain-mlogloss:0.0230876+0.000455044\ttest-mlogloss:0.0569502+0.00240074\n",
      "[1200]\ttrain-mlogloss:0.0225894+0.000440941\ttest-mlogloss:0.0568336+0.00243469\n",
      "[1225]\ttrain-mlogloss:0.0221184+0.000434039\ttest-mlogloss:0.0567478+0.00245175\n",
      "[1250]\ttrain-mlogloss:0.0216396+0.000435749\ttest-mlogloss:0.0566512+0.00246633\n",
      "[1275]\ttrain-mlogloss:0.0211928+0.000439028\ttest-mlogloss:0.0565832+0.00248233\n",
      "[1300]\ttrain-mlogloss:0.0207614+0.00042953\ttest-mlogloss:0.0565006+0.00252254\n",
      "[1325]\ttrain-mlogloss:0.020336+0.000429573\ttest-mlogloss:0.0564302+0.00254344\n",
      "[1350]\ttrain-mlogloss:0.01993+0.00041603\ttest-mlogloss:0.0563752+0.00255716\n",
      "[1375]\ttrain-mlogloss:0.0195298+0.00041434\ttest-mlogloss:0.0563222+0.00256132\n",
      "[1400]\ttrain-mlogloss:0.0191434+0.000401571\ttest-mlogloss:0.0562624+0.00257785\n",
      "[1425]\ttrain-mlogloss:0.018763+0.000389601\ttest-mlogloss:0.056232+0.00258965\n",
      "[1450]\ttrain-mlogloss:0.018392+0.000383813\ttest-mlogloss:0.056202+0.00259576\n",
      "[1475]\ttrain-mlogloss:0.0180324+0.000375539\ttest-mlogloss:0.0561784+0.00262241\n",
      "[1500]\ttrain-mlogloss:0.017673+0.000375705\ttest-mlogloss:0.056153+0.00263729\n",
      "[1525]\ttrain-mlogloss:0.0173404+0.000371076\ttest-mlogloss:0.0561408+0.00266857\n",
      "[1550]\ttrain-mlogloss:0.0170024+0.000362347\ttest-mlogloss:0.0561224+0.00267469\n",
      "[1575]\ttrain-mlogloss:0.0166876+0.000363308\ttest-mlogloss:0.0561232+0.0026779\n",
      "[1600]\ttrain-mlogloss:0.0163922+0.000376867\ttest-mlogloss:0.0561118+0.00269132\n",
      "[1625]\ttrain-mlogloss:0.0160916+0.000365169\ttest-mlogloss:0.0561052+0.00270347\n",
      "[1650]\ttrain-mlogloss:0.0158016+0.00036267\ttest-mlogloss:0.0561002+0.00271812\n",
      "[1675]\ttrain-mlogloss:0.0155202+0.000357791\ttest-mlogloss:0.0561142+0.00272911\n"
     ]
    }
   ],
   "source": [
    "final = xgb.cv(params,dtrain_res, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-011c4cdcb1cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# best_rounds_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfinal_round_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_rounds_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpreds_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpreds_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'high'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medium'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'low'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bst' is not defined"
     ]
    }
   ],
   "source": [
    "best_rounds_f = np.argmin(final['test-mlogloss-mean'])\n",
    "# best_rounds_f\n",
    "final_round_f = xgb.train(params, dtrain, best_rounds_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_f = final_round_f.predict(dtest)\n",
    "preds_f = pd.DataFrame(preds_f)\n",
    "cols = ['high', 'medium', 'low']\n",
    "preds_f.columns = cols\n",
    "preds_f['listing_id'] = listing_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_f.to_csv('my_preds_ens.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
